train policy models with pretrained: ./hpt-xlarge!
wandb url: https://wandb.ai/wanweijie2005-/hpt-transfer/runs/9go6uiv4
 >>>dataset_path: data/zarr_Robotwin_resnet_traj100_multiview load_from_cache: True
Robotwin size: 10327 episodes: 78 train: 70 eval: 8
data keys: KeysView(<zarr.hierarchy.Group '/data' read-only>)
Using device: cuda
load pretrained trunk config
load trunk from local disk
normalizer action stats min: Parameter containing:
tensor([-0.6862,  0.0000,  0.0000, -1.8003, -0.0697, -0.6838,  0.0000, -0.1090,
         0.0000,  0.0000, -1.6512, -0.2443, -0.0940,  0.0000])
normalizer action stats max: Parameter containing:
tensor([0.1486, 2.4550, 2.6869, 0.0000, 0.2623, 0.1395, 1.0000, 0.7470, 2.3881,
        2.4264, 0.0000, 0.0991, 0.7482, 1.0000])
normalizer state stats min: Parameter containing:
tensor([-0.6862,  0.0000,  0.0000, -1.8003, -0.0697, -0.6838,  0.0000, -0.1090,
         0.0000,  0.0000, -1.6512, -0.2443, -0.0940,  0.0000])
normalizer state stats max: Parameter containing:
tensor([0.1486, 2.4550, 2.6869, 0.0000, 0.2623, 0.1395, 1.0000, 0.7470, 2.3881,
        2.4264, 0.0000, 0.0991, 0.7482, 1.0000])
trunk frozen
==========================================
number of total params (M): 3.684 stem: 3.438 trunk: 0.000 head: 0.244 encoder: 0.000
{'seed': 42, 'output_dir': './output/hpt_train', 'domains': 'Robotwin', 'wb_tag': 'hpt_exp', 'log_interval': 10, 'script_name': 'hpt_train', 'pretrained_dir': './hpt-xlarge', 'parallel_eval': False, 'task_name': 'place_container_plate', 'task_config': 'demo_clean', 'episode_num': 100, 'processed_data_dir': './processed_data', 'total_num_traj': 100, 'dataset': {'_target_': 'hpt.dataset.local_traj_dataset.LocalTrajDataset', 'val_ratio': 0.1, 'pad_after': 0, 'episode_cnt': 100, 'step_cnt': 32000, 'data_augmentation': False, 'use_disk': True, 'pad_before': 0, 'data_ratio': 1, 'action_horizon': 8, 'observation_horizon': 5, 'dataset_postfix': '_traj100', 'image_encoder': 'resnet', 'dataset_encoder_postfix': '_resnet', 'dataset_name': 'Robotwin', 'precompute_feat': True, 'use_multiview': True, 'normalize_state': True, 'regenerate': False, 'action_multiple_horizon': True, 'random_mask_obs': True, 'data_augment_ratio': 1, 'proprioception_expand': False, 'proprioception_expand_dim': 32, 'env_rollout_fn': {'_target_': 'policy.HPT.process_data.convert_dataset_robotwin_cached', 'task_name': 'place_container_plate', 'task_config': 'demo_clean', 'episode_num': 100, 'use_cache': True, 'save_dir': './processed_data'}}, 'network': {'_target_': 'hpt.models.policy.Policy', 'embed_dim': 768, 'num_blocks': 32, 'num_heads': 8, 'drop_path': 0.1, 'use_modality_embedding': True, 'use_domain_embedding': False, 'observation_horizon': 5, 'action_horizon': 8, 'token_postprocessing': 'mean', 'cross_stem_attention': True, 'weight_init_style': 'pytorch', 'no_trunk': False, 'finetune_encoder': False}, 'stem': {'modalities': ['image', 'state'], 'modality_embed_dim': 768, 'normalize_state': True, 'state_embedding_dim': 14, 'cross_attention': True, 'precompute_feat': True, 'image_encoder': 'resnet', 'crossattn_dim_head': 64, 'crossattn_heads': 8, 'crossattn_modality_dropout': 0.1, 'num_blocks': 1, 'observation_horizon': 5, 'masked_autoencoding': False, 'random_horizon_masking': True, 'add_pos_embedding_to_state': False, 'crossattn_latent': {'image': 16, 'state': 16, 'language': 8}, 'image': {'_target_': 'hpt.models.policy_stem.MLP', 'input_dim': 512, 'output_dim': 768, 'widths': [128], 'num_of_copy': 5}, 'state': {'_target_': 'hpt.models.policy_stem.MLP', 'input_dim': 14, 'output_dim': 768, 'widths': [128]}}, 'head': {'_target_': 'hpt.models.policy_head.MLP', 'input_dim': 768, 'tanh_end': False, 'output_dim': 112, 'widths': [256, 128], 'normalize_action': False, 'dropout': True}, 'dataloader': {'batch_size': 256, 'num_workers': 4, 'pin_memory': True, 'persistent_workers': True, 'shuffle': True, 'drop_last': False}, 'val_dataloader': {'batch_size': 128, 'num_workers': 4, 'shuffle': False, 'pin_memory': True, 'persistent_workers': True, 'drop_last': False}, 'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 0.0001}, 'lr_scheduler': {'_target_': 'torch.optim.lr_scheduler.CosineAnnealingLR', 'T_max': 2000000, 'eta_min': 1e-06}, 'train': {'total_epochs': 1000, 'total_iters': 2000000, 'epoch_iters': 1000, 'validation_iters': 350, 'pretrained_dir': './hpt-xlarge', 'freeze_trunk': True, 'wandb_pretrained_dir': ''}, 'optimizer_misc': {'nontrunk_lr_scale': 1.0}, 'warmup_lr': {'lr': 1e-10, 'step': 1000}, 'gpu_id': 4}
Epoch size: 41 Traj: 78 Train: 10327 Test: 1200
  0%|                                                                                                                                                          | 0/1000 [00:00<?, ?it/s]








Epoch: 0 40 Step: 40/41 Time: 0.0740.168 Loss: 0.719 Grad: 0.060: 100%|█████████████████████████████████████████████████████████████████████████████████| 41/41 [00:20<00:00,  2.84it/s]

Test Epoch: 0 Step: 7 Domain: Robotwin Loss: 0.632:  70%|███████████████████████████████████████████████████████████████████▏                            | 7/10 [00:02<00:00,  3.93it/s]

  0%|▏                                                                                                                                               | 1/1000 [00:28<7:47:01, 28.05s/it]









  0%|▎                                                                                                                                               | 2/1000 [00:49<6:40:16, 24.06s/it]
  0%|                                                                                                                                                            | 0/41 [00:00<?, ?it/s]








  0%|▍                                                                                                                                               | 3/1000 [01:09<6:08:28, 22.18s/it]
  0%|                                                                                                                                                            | 0/41 [00:00<?, ?it/s]









Epoch: 3 161 Step: 38/41 Time: 0.1870.159 Loss: 0.498 Grad: 0.011:  95%|████████████████████████████████████████████████████████████████████████████    | 39/41 [00:20<00:00,  2.42it/s]
  0%|▌                                                                                                                                               | 4/1000 [01:30<6:02:32, 21.84s/it]








  0%|▋                                                                                                                                               | 5/1000 [01:49<5:47:41, 20.97s/it]
  0%|                                                                                                                                                            | 0/41 [00:00<?, ?it/s]









Epoch: 5 245 Step: 40/41 Time: 0.0720.170 Loss: 0.410 Grad: 0.015: 100%|████████████████████████████████████████████████████████████████████████████████| 41/41 [00:20<00:00,  3.14it/s]
  1%|▊                                                                                                                                               | 6/1000 [02:10<5:43:37, 20.74s/it]









  1%|█                                                                                                                                               | 7/1000 [02:31<5:45:19, 20.87s/it]
  0%|                                                                                                                                                            | 0/41 [00:00<?, ?it/s]

  1%|█                                                                                                                                               | 7/1000 [02:36<6:09:43, 22.34s/it]
Traceback (most recent call last):
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/train.py", line 171, in <module>
    main()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/train.py", line 143, in main
    train_stats = train(cfg.log_interval, policy, device, train_loader, optimizer, scheduler, epoch)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/HPT/hpt/train_test.py", line 94, in train
    domain_loss.backward()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/train.py", line 171, in <module>
    main()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/train.py", line 143, in main
    train_stats = train(cfg.log_interval, policy, device, train_loader, optimizer, scheduler, epoch)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/HPT/hpt/train_test.py", line 94, in train
    domain_loss.backward()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt