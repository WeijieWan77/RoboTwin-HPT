2025-08-31 13:05:53,273 INFO    MainThread:86655 [wandb_setup.py:_flush():76] Current SDK version is 0.16.1
2025-08-31 13:05:53,273 INFO    MainThread:86655 [wandb_setup.py:_flush():76] Configure stats pid to 86655
2025-08-31 13:05:53,273 INFO    MainThread:86655 [wandb_setup.py:_flush():76] Loading settings from /home/lumina/.config/wandb/settings
2025-08-31 13:05:53,273 INFO    MainThread:86655 [wandb_setup.py:_flush():76] Loading settings from /data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/wandb/settings
2025-08-31 13:05:53,273 INFO    MainThread:86655 [wandb_setup.py:_flush():76] Loading settings from environment variables: {}
2025-08-31 13:05:53,273 INFO    MainThread:86655 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2025-08-31 13:05:53,273 INFO    MainThread:86655 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'RoboTwin/policy/HPT/train.py', 'program_abspath': '/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/train.py', 'program': '/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/train.py'}
2025-08-31 13:05:53,273 INFO    MainThread:86655 [wandb_init.py:_log_setup():524] Logging user logs to /data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/wandb/run-20250831_130553-yi3xrk9o/logs/debug.log
2025-08-31 13:05:53,277 INFO    MainThread:86655 [wandb_init.py:_log_setup():525] Logging internal logs to /data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/wandb/run-20250831_130553-yi3xrk9o/logs/debug-internal.log
2025-08-31 13:05:53,277 INFO    MainThread:86655 [wandb_init.py:init():564] calling init triggers
2025-08-31 13:05:53,277 INFO    MainThread:86655 [wandb_init.py:init():571] wandb.init called with sweep_config: {}
config: {'seed': 42, 'output_dir': './output/hpt_train', 'domains': 'Robotwin', 'wb_tag': 'hpt_exp', 'log_interval': 10, 'script_name': 'hpt_train', 'pretrained_dir': './hpt-xlarge', 'parallel_eval': False, 'task_name': 'place_object_scale', 'task_config': 'demo_clean', 'episode_num': 50, 'processed_data_dir': './processed_data', 'total_num_traj': 100, 'dataset': {'_target_': 'hpt.dataset.local_traj_dataset.LocalTrajDataset', 'val_ratio': 0.1, 'pad_after': 0, 'episode_cnt': 50, 'step_cnt': 32000, 'data_augmentation': False, 'use_disk': True, 'pad_before': 0, 'data_ratio': 1, 'action_horizon': 8, 'observation_horizon': 5, 'dataset_postfix': '_traj50', 'image_encoder': 'resnet', 'dataset_encoder_postfix': '_resnet', 'dataset_name': 'Robotwin', 'precompute_feat': True, 'use_multiview': True, 'normalize_state': True, 'regenerate': False, 'action_multiple_horizon': True, 'random_mask_obs': True, 'data_augment_ratio': 1, 'proprioception_expand': False, 'proprioception_expand_dim': 32, 'env_rollout_fn': {'_target_': 'policy.HPT.process_data.convert_dataset_robotwin_cached', 'task_name': 'place_object_scale', 'task_config': 'demo_clean', 'episode_num': 50, 'use_cache': True, 'save_dir': './processed_data'}}, 'network': {'_target_': 'hpt.models.policy.Policy', 'embed_dim': 768, 'num_blocks': 16, 'num_heads': 8, 'drop_path': 0.1, 'use_modality_embedding': True, 'use_domain_embedding': False, 'observation_horizon': 5, 'action_horizon': 8, 'token_postprocessing': 'mean', 'cross_stem_attention': True, 'weight_init_style': 'pytorch', 'no_trunk': False, 'finetune_encoder': False}, 'stem': {'modalities': ['image', 'state'], 'modality_embed_dim': 768, 'normalize_state': True, 'state_embedding_dim': 14, 'cross_attention': True, 'precompute_feat': True, 'image_encoder': 'resnet', 'crossattn_dim_head': 64, 'crossattn_heads': 8, 'crossattn_modality_dropout': 0.1, 'num_blocks': 1, 'observation_horizon': 5, 'masked_autoencoding': False, 'random_horizon_masking': True, 'add_pos_embedding_to_state': False, 'crossattn_latent': {'image': 16, 'state': 16, 'language': 8}, 'image': {'_target_': 'hpt.models.policy_stem.MLP', 'input_dim': 512, 'output_dim': 768, 'widths': [128], 'num_of_copy': 1}, 'state': {'_target_': 'hpt.models.policy_stem.MLP', 'input_dim': 14, 'output_dim': 768, 'widths': [128]}}, 'head': {'_target_': 'hpt.models.policy_head.MLP', 'input_dim': 768, 'tanh_end': False, 'output_dim': 112, 'widths': [256, 128], 'normalize_action': False, 'dropout': True}, 'dataloader': {'batch_size': 128, 'num_workers': 4, 'pin_memory': True, 'persistent_workers': True, 'shuffle': True, 'drop_last': False}, 'val_dataloader': {'batch_size': 128, 'num_workers': 4, 'shuffle': False, 'pin_memory': True, 'persistent_workers': True, 'drop_last': False}, 'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 0.0001}, 'lr_scheduler': {'_target_': 'torch.optim.lr_scheduler.CosineAnnealingLR', 'T_max': 1000, 'eta_min': 1e-06}, 'train': {'total_epochs': 1000, 'total_iters': 2000000, 'epoch_iters': 1000, 'validation_iters': 350, 'pretrained_dir': './hpt-xlarge', 'freeze_trunk': True, 'wandb_pretrained_dir': ''}, 'optimizer_misc': {'nontrunk_lr_scale': 1.0}, 'warmup_lr': {'lr': 1e-10, 'step': 1000}, 'gpu_id': 4}
2025-08-31 13:05:53,277 INFO    MainThread:86655 [wandb_init.py:init():614] starting backend
2025-08-31 13:05:53,277 INFO    MainThread:86655 [wandb_init.py:init():618] setting up manager
2025-08-31 13:05:53,279 INFO    MainThread:86655 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2025-08-31 13:05:53,280 INFO    MainThread:86655 [wandb_init.py:init():624] backend started and connected
2025-08-31 13:05:53,283 INFO    MainThread:86655 [wandb_init.py:init():716] updated telemetry
2025-08-31 13:05:53,289 INFO    MainThread:86655 [wandb_init.py:init():749] communicating run to backend with 90.0 second timeout
2025-08-31 13:05:55,543 INFO    MainThread:86655 [wandb_run.py:_on_init():2254] communicating current version
2025-08-31 13:05:55,613 INFO    MainThread:86655 [wandb_run.py:_on_init():2263] got version response 
2025-08-31 13:05:55,613 INFO    MainThread:86655 [wandb_init.py:init():800] starting run threads in backend
2025-08-31 13:06:00,187 INFO    MainThread:86655 [wandb_run.py:_console_start():2233] atexit reg
2025-08-31 13:06:00,188 INFO    MainThread:86655 [wandb_run.py:_redirect():2088] redirect: wrap_raw
2025-08-31 13:06:00,188 INFO    MainThread:86655 [wandb_run.py:_redirect():2153] Wrapping output streams.
2025-08-31 13:06:00,188 INFO    MainThread:86655 [wandb_run.py:_redirect():2178] Redirects installed.
2025-08-31 13:06:00,189 INFO    MainThread:86655 [wandb_init.py:init():841] run started, returning control to user process
2025-08-31 13:06:15,270 WARNING MsgRouterThr:86655 [router.py:message_loop():77] message_loop has been closed
