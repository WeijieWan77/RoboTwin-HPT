train policy models with pretrained: ./hpt-xlarge!
wandb url: https://wandb.ai/wanweijie2005-/hpt-transfer/runs/8baw3xts
 >>>dataset_path: data/zarr_Robotwin_resnet_traj100_multiview load_from_cache: True
Robotwin size: 10327 episodes: 78 train: 70 eval: 8
data keys: KeysView(<zarr.hierarchy.Group '/data' read-only>)
Using device: cuda
load pretrained trunk config
[34m[1mwandb[39m[22m: Network error resolved after 0:00:17.650899, resuming normal operation.
load trunk from local disk
normalizer action stats min: Parameter containing:
tensor([-0.6862,  0.0000,  0.0000, -1.8003, -0.0697, -0.6838,  0.0000, -0.1090,
         0.0000,  0.0000, -1.6512, -0.2443, -0.0940,  0.0000])
normalizer action stats max: Parameter containing:
tensor([0.1486, 2.4550, 2.6869, 0.0000, 0.2623, 0.1395, 1.0000, 0.7470, 2.3881,
        2.4264, 0.0000, 0.0991, 0.7482, 1.0000])
normalizer state stats min: Parameter containing:
tensor([-0.6862,  0.0000,  0.0000, -1.8003, -0.0697, -0.6838,  0.0000, -0.1090,
         0.0000,  0.0000, -1.6512, -0.2443, -0.0940,  0.0000])
normalizer state stats max: Parameter containing:
tensor([0.1486, 2.4550, 2.6869, 0.0000, 0.2623, 0.1395, 1.0000, 0.7470, 2.3881,
        2.4264, 0.0000, 0.0991, 0.7482, 1.0000])
trunk frozen
==========================================
number of total params (M): 3.684 stem: 3.438 trunk: 0.000 head: 0.244 encoder: 0.000
{'seed': 42, 'output_dir': './output/hpt_train', 'domains': 'Robotwin', 'wb_tag': 'hpt_exp', 'log_interval': 10, 'script_name': 'hpt_train', 'pretrained_dir': './hpt-xlarge', 'parallel_eval': False, 'task_name': 'place_container_plate', 'task_config': 'demo_clean', 'episode_num': 100, 'processed_data_dir': './processed_data', 'total_num_traj': 100, 'dataset': {'_target_': 'hpt.dataset.local_traj_dataset.LocalTrajDataset', 'val_ratio': 0.1, 'pad_after': 0, 'episode_cnt': 100, 'step_cnt': 32000, 'data_augmentation': False, 'use_disk': True, 'pad_before': 0, 'data_ratio': 1, 'action_horizon': 8, 'observation_horizon': 5, 'dataset_postfix': '_traj100', 'image_encoder': 'resnet', 'dataset_encoder_postfix': '_resnet', 'dataset_name': 'Robotwin', 'precompute_feat': True, 'use_multiview': True, 'normalize_state': True, 'regenerate': False, 'action_multiple_horizon': True, 'random_mask_obs': True, 'data_augment_ratio': 1, 'proprioception_expand': False, 'proprioception_expand_dim': 32, 'env_rollout_fn': {'_target_': 'policy.HPT.process_data.convert_dataset_robotwin_cached', 'task_name': 'place_container_plate', 'task_config': 'demo_clean', 'episode_num': 100, 'use_cache': True, 'save_dir': './processed_data'}}, 'network': {'_target_': 'hpt.models.policy.Policy', 'embed_dim': 768, 'num_blocks': 32, 'num_heads': 8, 'drop_path': 0.1, 'use_modality_embedding': True, 'use_domain_embedding': False, 'observation_horizon': 5, 'action_horizon': 8, 'token_postprocessing': 'mean', 'cross_stem_attention': True, 'weight_init_style': 'pytorch', 'no_trunk': False, 'finetune_encoder': False}, 'stem': {'modalities': ['image', 'state'], 'modality_embed_dim': 768, 'normalize_state': True, 'state_embedding_dim': 14, 'cross_attention': True, 'precompute_feat': True, 'image_encoder': 'resnet', 'crossattn_dim_head': 64, 'crossattn_heads': 8, 'crossattn_modality_dropout': 0.1, 'num_blocks': 1, 'observation_horizon': 5, 'masked_autoencoding': False, 'random_horizon_masking': True, 'add_pos_embedding_to_state': False, 'crossattn_latent': {'image': 16, 'state': 16, 'language': 8}, 'image': {'_target_': 'hpt.models.policy_stem.MLP', 'input_dim': 512, 'output_dim': 768, 'widths': [128], 'num_of_copy': 5}, 'state': {'_target_': 'hpt.models.policy_stem.MLP', 'input_dim': 14, 'output_dim': 768, 'widths': [128]}}, 'head': {'_target_': 'hpt.models.policy_head.MLP', 'input_dim': 768, 'tanh_end': False, 'output_dim': 112, 'widths': [256, 128], 'normalize_action': False, 'dropout': True}, 'dataloader': {'batch_size': 256, 'num_workers': 4, 'pin_memory': True, 'persistent_workers': True, 'shuffle': True, 'drop_last': False}, 'val_dataloader': {'batch_size': 128, 'num_workers': 4, 'shuffle': False, 'pin_memory': True, 'persistent_workers': True, 'drop_last': False}, 'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 0.0001}, 'lr_scheduler': {'_target_': 'torch.optim.lr_scheduler.CosineAnnealingLR', 'T_max': 2000000, 'eta_min': 1e-06}, 'train': {'total_epochs': 1000, 'total_iters': 2000000, 'epoch_iters': 1000, 'validation_iters': 350, 'pretrained_dir': './hpt-xlarge', 'freeze_trunk': True, 'wandb_pretrained_dir': ''}, 'optimizer_misc': {'nontrunk_lr_scale': 1.0}, 'warmup_lr': {'lr': 1e-10, 'step': 1000}, 'gpu_id': 4}
Epoch size: 41 Traj: 78 Train: 10327 Test: 1200
  0%|                                                                                                                                                          | 0/1000 [00:00<?, ?it/s]







Epoch: 0 40 Step: 40/41 Time: 0.0750.168 Loss: 0.719 Grad: 0.060: 100%|█████████████████████████████████████████████████████████████████████████████████| 41/41 [00:20<00:00,  2.80it/s]
Test Epoch: 0 Step: 3 Domain: Robotwin Loss: 0.565:  30%|████████████████████████████▊                                                                   | 3/10 [00:01<00:03,  2.27it/s]

  0%|▏                                                                                                                                               | 1/1000 [00:27<7:43:33, 27.84s/it]








Epoch: 1 76 Step: 35/41 Time: 0.1820.161 Loss: 0.641 Grad: 0.032:  88%|███████████████████████████████████████████████████████████████████████          | 36/41 [00:17<00:01,  2.55it/s]
  0%|▎                                                                                                                                               | 2/1000 [00:47<6:26:47, 23.25s/it]








Epoch: 2 116 Step: 34/41 Time: 0.1930.164 Loss: 0.570 Grad: 0.015:  85%|████████████████████████████████████████████████████████████████████▎           | 35/41 [00:18<00:02,  2.25it/s]
  0%|▍                                                                                                                                               | 3/1000 [01:08<6:03:39, 21.89s/it]









Epoch: 3 162 Step: 39/41 Time: 0.1810.165 Loss: 0.500 Grad: 0.012:  98%|██████████████████████████████████████████████████████████████████████████████  | 40/41 [00:20<00:00,  2.51it/s]
  0%|▌                                                                                                                                               | 4/1000 [01:28<5:55:15, 21.40s/it]









Epoch: 4 201 Step: 37/41 Time: 0.1820.164 Loss: 0.450 Grad: 0.017:  93%|██████████████████████████████████████████████████████████████████████████▏     | 38/41 [00:19<00:01,  2.02it/s]
  0%|▋                                                                                                                                               | 5/1000 [01:49<5:50:45, 21.15s/it]









Epoch: 5 243 Step: 38/41 Time: 0.1930.170 Loss: 0.422 Grad: 0.013:  95%|████████████████████████████████████████████████████████████████████████████    | 39/41 [00:19<00:00,  2.43it/s]
  1%|▊                                                                                                                                               | 6/1000 [02:09<5:43:35, 20.74s/it]









Epoch: 6 285 Step: 39/41 Time: 0.1770.428 Loss: 0.383 Grad: 0.015:  98%|██████████████████████████████████████████████████████████████████████████████  | 40/41 [00:19<00:00,  2.17it/s]
  1%|█                                                                                                                                               | 7/1000 [02:29<5:37:41, 20.40s/it]









Epoch: 7 325 Step: 38/41 Time: 0.1870.162 Loss: 0.344 Grad: 0.026:  95%|████████████████████████████████████████████████████████████████████████████    | 39/41 [00:19<00:00,  2.41it/s]
  1%|█▏                                                                                                                                              | 8/1000 [02:49<5:34:43, 20.25s/it]









Epoch: 8 363 Step: 35/41 Time: 0.1880.171 Loss: 0.320 Grad: 0.016:  88%|██████████████████████████████████████████████████████████████████████▏         | 36/41 [00:19<00:02,  2.15it/s]
  1%|█▎                                                                                                                                              | 9/1000 [03:10<5:39:58, 20.58s/it]









Epoch: 9 408 Step: 39/41 Time: 0.1830.173 Loss: 0.293 Grad: 0.019:  98%|██████████████████████████████████████████████████████████████████████████████  | 40/41 [00:20<00:00,  2.49it/s]
  1%|█▍                                                                                                                                             | 10/1000 [03:31<5:39:58, 20.60s/it]









Epoch: 10 450 Step: 40/41 Time: 0.0720.173 Loss: 0.276 Grad: 0.025: 100%|███████████████████████████████████████████████████████████████████████████████| 41/41 [00:20<00:00,  3.07it/s]

  0%|                                                                                                                                                            | 0/10 [00:00<?, ?it/s]
  1%|█▌                                                                                                                                             | 11/1000 [03:59<6:17:27, 22.90s/it]








  1%|█▋                                                                                                                                             | 12/1000 [04:18<6:00:28, 21.89s/it]
  0%|                                                                                                                                                            | 0/41 [00:00<?, ?it/s]









Epoch: 12 530 Step: 38/41 Time: 0.1990.170 Loss: 0.231 Grad: 0.023:  95%|███████████████████████████████████████████████████████████████████████████▏   | 39/41 [00:20<00:00,  2.35it/s]
  1%|█▊                                                                                                                                             | 13/1000 [04:39<5:54:25, 21.55s/it]










Epoch: 13 568 Step: 35/41 Time: 0.1830.170 Loss: 0.211 Grad: 0.015:  88%|█████████████████████████████████████████████████████████████████████▎         | 36/41 [00:21<00:02,  2.10it/s]
  1%|██                                                                                                                                             | 14/1000 [05:02<6:02:33, 22.06s/it]

  1%|██                                                                                                                                             | 14/1000 [05:08<6:02:15, 22.04s/it]
Traceback (most recent call last):
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/train.py", line 171, in <module>
    main()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/train.py", line 143, in main
    train_stats = train(cfg.log_interval, policy, device, train_loader, optimizer, scheduler, epoch)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/HPT/hpt/train_test.py", line 88, in train
    for batch_idx, batch in enumerate(pbar):
  File "/home/lumina/.local/lib/python3.10/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1283, in _get_data
    success, data = self._try_get_data()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/queue.py", line 180, in get
    self.not_empty.wait(remaining)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/threading.py", line 324, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt
Traceback (most recent call last):
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/train.py", line 171, in <module>
    main()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/train.py", line 143, in main
    train_stats = train(cfg.log_interval, policy, device, train_loader, optimizer, scheduler, epoch)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/HPT/hpt/train_test.py", line 88, in train
    for batch_idx, batch in enumerate(pbar):
  File "/home/lumina/.local/lib/python3.10/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1283, in _get_data
    success, data = self._try_get_data()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/queue.py", line 180, in get
    self.not_empty.wait(remaining)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/threading.py", line 324, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt