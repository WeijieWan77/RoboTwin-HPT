train policy models with pretrained: ./hpt-xlarge!
wandb url: https://wandb.ai/wanweijie2005-/hpt-transfer/runs/l20riy04
 >>>dataset_path: data/zarr_Robotwin_resnet_traj50_multiview load_from_cache: True
Robotwin size: 6086 episodes: 50 train: 45 eval: 5
data keys: KeysView(<zarr.hierarchy.Group '/data' read-only>)
Using device: cuda
load pretrained trunk config
load trunk from local disk
normalizer action stats min: Parameter containing:
tensor([-0.8762,  0.0000,  0.0000, -1.7300, -0.0406, -2.3191,  0.0000,  0.0000,
         0.0000,  0.0000, -1.8668, -0.0369, -2.9199,  0.0000])
normalizer action stats max: Parameter containing:
tensor([0.0000, 2.4783, 2.6188, 0.0000, 0.0772, 2.5356, 1.0000, 0.8395, 2.6848,
        3.0897, 0.0000, 0.0630, 1.2573, 1.0000])
normalizer state stats min: Parameter containing:
tensor([-0.8762,  0.0000,  0.0000, -1.7300, -0.0406, -2.3191,  0.0000,  0.0000,
         0.0000,  0.0000, -1.8668, -0.0369, -2.9199,  0.0000])
normalizer state stats max: Parameter containing:
tensor([0.0000, 2.4783, 2.6188, 0.0000, 0.0772, 2.5356, 1.0000, 0.8395, 2.6848,
        3.0897, 0.0000, 0.0630, 1.2573, 1.0000])
trunk frozen
==========================================
number of total params (M): 3.684 stem: 3.438 trunk: 0.000 head: 0.244 encoder: 0.000
====================================================
========== Key Runtime Hyperparameters ===========
====================================================
--- Dataset ---
Task Name:                place_object_scale
Task Config:              demo_clean
Episode Count:            50
Observation Horizon:      5
Action Horizon:           8
--- Network (Trunk) ---
Embedding Dim:            768
Num Blocks:               16
Num Heads:                8
--- Stem ---
Modalities:               ['image', 'state']
Image Tokens:             16
State Tokens:             16
--- Head ---
Head Type:                MLP
Output Dim:               112
Tanh End:                 False
--- Training ---
Batch Size:               128
Learning Rate:            0.0001
Total Iters:              2000000
Freeze Trunk:             True
====================================================
  0%|                                                                                                                                                                                               | 0/1000 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                 | 0/48 [00:00<?, ?it/s]
{'seed': 42, 'output_dir': './output/hpt_train', 'domains': 'Robotwin', 'wb_tag': 'hpt_exp', 'log_interval': 10, 'script_name': 'hpt_train', 'pretrained_dir': './hpt-xlarge', 'parallel_eval': False, 'task_name': 'place_object_scale', 'task_config': 'demo_clean', 'episode_num': 50, 'processed_data_dir': './processed_data', 'total_num_traj': 100, 'dataset': {'_target_': 'hpt.dataset.local_traj_dataset.LocalTrajDataset', 'val_ratio': 0.1, 'pad_after': 0, 'episode_cnt': 50, 'step_cnt': 32000, 'data_augmentation': False, 'use_disk': True, 'pad_before': 0, 'data_ratio': 1, 'action_horizon': 8, 'observation_horizon': 5, 'dataset_postfix': '_traj50', 'image_encoder': 'resnet', 'dataset_encoder_postfix': '_resnet', 'dataset_name': 'Robotwin', 'precompute_feat': True, 'use_multiview': True, 'normalize_state': True, 'regenerate': False, 'action_multiple_horizon': True, 'random_mask_obs': True, 'data_augment_ratio': 1, 'proprioception_expand': False, 'proprioception_expand_dim': 32, 'env_rollout_fn': {'_target_': 'policy.HPT.process_data.convert_dataset_robotwin_cached', 'task_name': 'place_object_scale', 'task_config': 'demo_clean', 'episode_num': 50, 'use_cache': True, 'save_dir': './processed_data'}}, 'network': {'_target_': 'hpt.models.policy.Policy', 'embed_dim': 768, 'num_blocks': 16, 'num_heads': 8, 'drop_path': 0.1, 'use_modality_embedding': True, 'use_domain_embedding': False, 'observation_horizon': 5, 'action_horizon': 8, 'token_postprocessing': 'mean', 'cross_stem_attention': True, 'weight_init_style': 'pytorch', 'no_trunk': False, 'finetune_encoder': False}, 'stem': {'modalities': ['image', 'state'], 'modality_embed_dim': 768, 'normalize_state': True, 'state_embedding_dim': 14, 'cross_attention': True, 'precompute_feat': True, 'image_encoder': 'resnet', 'crossattn_dim_head': 64, 'crossattn_heads': 8, 'crossattn_modality_dropout': 0.1, 'num_blocks': 1, 'observation_horizon': 5, 'masked_autoencoding': False, 'random_horizon_masking': True, 'add_pos_embedding_to_state': False, 'crossattn_latent': {'image': 16, 'state': 16, 'language': 8}, 'image': {'_target_': 'hpt.models.policy_stem.MLP', 'input_dim': 512, 'output_dim': 768, 'widths': [128], 'num_of_copy': 5}, 'state': {'_target_': 'hpt.models.policy_stem.MLP', 'input_dim': 14, 'output_dim': 768, 'widths': [128]}}, 'head': {'_target_': 'hpt.models.policy_head.MLP', 'input_dim': 768, 'tanh_end': False, 'output_dim': 112, 'widths': [256, 128], 'normalize_action': False, 'dropout': True}, 'dataloader': {'batch_size': 128, 'num_workers': 4, 'pin_memory': True, 'persistent_workers': True, 'shuffle': True, 'drop_last': False}, 'val_dataloader': {'batch_size': 128, 'num_workers': 4, 'shuffle': False, 'pin_memory': True, 'persistent_workers': True, 'drop_last': False}, 'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 0.0001}, 'lr_scheduler': {'_target_': 'torch.optim.lr_scheduler.CosineAnnealingLR', 'T_max': 2000000, 'eta_min': 1e-06}, 'train': {'total_epochs': 1000, 'total_iters': 2000000, 'epoch_iters': 1000, 'validation_iters': 350, 'pretrained_dir': './hpt-xlarge', 'freeze_trunk': True, 'wandb_pretrained_dir': ''}, 'optimizer_misc': {'nontrunk_lr_scale': 1.0}, 'warmup_lr': {'lr': 1e-10, 'step': 1000}, 'gpu_id': 4}
  0%|                                                                                                                                                                                               | 0/1000 [00:02<?, ?it/s]
Error executing job with overrides: []
Traceback (most recent call last):
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/train.py", line 213, in <module>
    main()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/train.py", line 185, in main
    train_stats = train(cfg.log_interval, policy, device, train_loader, optimizer, scheduler, epoch)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/HPT/hpt/train_test.py", line 92, in train
    domain_loss = model.compute_loss(batch)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/HPT/hpt/models/policy.py", line 302, in compute_loss
    features = self.forward_features(domain, data)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/HPT/hpt/models/policy.py", line 292, in forward_features
    self.trunk_tokens = self.trunk["trunk"](self.trunk_tokens)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/HPT/hpt/models/transformer.py", line 312, in forward
    tokens = blk(tokens, attn_mask=attn_mask)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/HPT/hpt/models/transformer.py", line 219, in forward
    x = x + self.drop_path(self.mlp(self.norm_2(x)))
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/HPT/hpt/models/transformer.py", line 155, in forward
    x = self.fc1(x)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 117, in forward
    return F.linear(input, self.weight, self.bias)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 50.19 MiB is free. Process 3656181 has 18.29 GiB memory in use. Including non-PyTorch memory, this process has 5.27 GiB memory in use. Of the allocated memory 4.52 GiB is allocated by PyTorch, and 309.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/train.py", line 213, in <module>
    main()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/train.py", line 185, in main
    train_stats = train(cfg.log_interval, policy, device, train_loader, optimizer, scheduler, epoch)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/HPT/hpt/train_test.py", line 92, in train
    domain_loss = model.compute_loss(batch)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/HPT/hpt/models/policy.py", line 302, in compute_loss
    features = self.forward_features(domain, data)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/HPT/hpt/models/policy.py", line 292, in forward_features
    self.trunk_tokens = self.trunk["trunk"](self.trunk_tokens)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/HPT/hpt/models/transformer.py", line 312, in forward
    tokens = blk(tokens, attn_mask=attn_mask)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/HPT/hpt/models/transformer.py", line 219, in forward
    x = x + self.drop_path(self.mlp(self.norm_2(x)))
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/HPT/hpt/models/transformer.py", line 155, in forward
    x = self.fc1(x)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lumina/miniconda3/envs/weijie_hpt/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 117, in forward
    return F.linear(input, self.weight, self.bias)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 50.19 MiB is free. Process 3656181 has 18.29 GiB memory in use. Including non-PyTorch memory, this process has 5.27 GiB memory in use. Of the allocated memory 4.52 GiB is allocated by PyTorch, and 309.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)