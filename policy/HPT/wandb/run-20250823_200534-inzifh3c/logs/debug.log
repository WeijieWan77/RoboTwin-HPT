2025-08-23 20:05:34,267 INFO    MainThread:3777087 [wandb_setup.py:_flush():76] Current SDK version is 0.16.1
2025-08-23 20:05:34,267 INFO    MainThread:3777087 [wandb_setup.py:_flush():76] Configure stats pid to 3777087
2025-08-23 20:05:34,267 INFO    MainThread:3777087 [wandb_setup.py:_flush():76] Loading settings from /home/lumina/.config/wandb/settings
2025-08-23 20:05:34,267 INFO    MainThread:3777087 [wandb_setup.py:_flush():76] Loading settings from /data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/wandb/settings
2025-08-23 20:05:34,267 INFO    MainThread:3777087 [wandb_setup.py:_flush():76] Loading settings from environment variables: {}
2025-08-23 20:05:34,267 INFO    MainThread:3777087 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2025-08-23 20:05:34,267 INFO    MainThread:3777087 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'RoboTwin/policy/HPT/train.py', 'program_abspath': '/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/train.py', 'program': '/data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/train.py'}
2025-08-23 20:05:34,267 INFO    MainThread:3777087 [wandb_init.py:_log_setup():524] Logging user logs to /data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/wandb/run-20250823_200534-inzifh3c/logs/debug.log
2025-08-23 20:05:34,267 INFO    MainThread:3777087 [wandb_init.py:_log_setup():525] Logging internal logs to /data0/lumina/weijie/Robotwin_hpt/RoboTwin/policy/HPT/wandb/run-20250823_200534-inzifh3c/logs/debug-internal.log
2025-08-23 20:05:34,267 INFO    MainThread:3777087 [wandb_init.py:init():564] calling init triggers
2025-08-23 20:05:34,267 INFO    MainThread:3777087 [wandb_init.py:init():571] wandb.init called with sweep_config: {}
config: {'seed': 0, 'output_dir': './output/hpt_train', 'domains': 'Robotwin', 'wb_tag': 'hpt_exp', 'log_interval': 10, 'script_name': 'hpt_train', 'pretrained_dir': './hpt-xlarge', 'parallel_eval': False, 'task_name': 'place_object_scale', 'task_config': 'demo_clean', 'episode_num': 50, 'processed_data_dir': './processed_data', 'total_num_traj': 100, 'dataset': {'_target_': 'hpt.dataset.local_traj_dataset.LocalTrajDataset', 'horizon': 1, 'val_ratio': 0.1, 'pad_after': 0, 'episode_cnt': 50, 'step_cnt': 100000, 'data_augmentation': False, 'use_disk': False, 'pad_before': 0, 'data_ratio': 1, 'action_horizon': 8, 'observation_horizon': 4, 'dataset_postfix': '_traj50', 'image_encoder': 'resnet', 'dataset_encoder_postfix': '_resnet', 'dataset_name': 'Robotwin', 'precompute_feat': True, 'use_multiview': True, 'normalize_state': True, 'regenerate': False, 'action_multiple_horizon': True, 'random_mask_obs': True, 'data_augment_ratio': 1, 'proprioception_expand': False, 'proprioception_expand_dim': 32, 'env_rollout_fn': {'_target_': 'policy.HPT.process_data.convert_dataset_robotwin_cached', 'task_name': 'place_object_scale', 'task_config': 'demo_clean', 'episode_num': 50, 'use_cache': True, 'save_dir': './processed_data'}}, 'network': {'_target_': 'hpt.models.policy.Policy', 'embed_dim': 768, 'num_blocks': 16, 'num_heads': 8, 'drop_path': 0.1, 'use_modality_embedding': True, 'use_domain_embedding': False, 'observation_horizon': 1, 'action_horizon': 1, 'token_postprocessing': 'mean', 'cross_stem_attention': True, 'weight_init_style': 'pytorch', 'no_trunk': False, 'finetune_encoder': False}, 'stem': {'modalities': ['image', 'state'], 'modality_embed_dim': 768, 'normalize_state': True, 'state_embedding_dim': 14, 'cross_attention': True, 'precompute_feat': True, 'image_encoder': 'resnet', 'crossattn_dim_head': 64, 'crossattn_heads': 8, 'crossattn_modality_dropout': 0.1, 'num_blocks': 1, 'observation_horizon': 4, 'masked_autoencoding': False, 'random_horizon_masking': True, 'add_pos_embedding_to_state': False, 'crossattn_latent': {'image': 16, 'state': 16, 'language': 8}, 'image': {'_target_': 'hpt.models.policy_stem.MLP', 'input_dim': 512, 'output_dim': 768, 'widths': [128], 'num_of_copy': 1}, 'state': {'_target_': 'hpt.models.policy_stem.MLP', 'input_dim': 14, 'output_dim': 768, 'widths': [128]}}, 'head': {'_target_': 'hpt.models.policy_head.MLP', 'input_dim': 768, 'tanh_end': True, 'output_dim': 112, 'widths': [256, 128], 'normalize_action': True, 'dropout': True}, 'dataloader': {'batch_size': 256, 'num_workers': 1, 'pin_memory': True, 'persistent_workers': True, 'shuffle': True, 'drop_last': False}, 'val_dataloader': {'batch_size': 256, 'num_workers': 1, 'shuffle': False, 'pin_memory': True, 'persistent_workers': True, 'drop_last': False}, 'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 0.0001}, 'lr_scheduler': {'_target_': 'torch.optim.lr_scheduler.CosineAnnealingLR', 'T_max': 100, 'eta_min': 1e-06}, 'train': {'total_epochs': 100, 'total_iters': 20000, 'epoch_iters': 2000, 'validation_iters': 100, 'pretrained_dir': './hpt-xlarge', 'freeze_trunk': True, 'wandb_pretrained_dir': ''}, 'optimizer_misc': {'nontrunk_lr_scale': 1.0}, 'warmup_lr': {'lr': 1e-10, 'step': 1000}, 'gpu_id': 4}
2025-08-23 20:05:34,267 INFO    MainThread:3777087 [wandb_init.py:init():614] starting backend
2025-08-23 20:05:34,267 INFO    MainThread:3777087 [wandb_init.py:init():618] setting up manager
2025-08-23 20:05:34,269 INFO    MainThread:3777087 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2025-08-23 20:05:34,270 INFO    MainThread:3777087 [wandb_init.py:init():624] backend started and connected
2025-08-23 20:05:34,273 INFO    MainThread:3777087 [wandb_init.py:init():716] updated telemetry
2025-08-23 20:05:34,278 INFO    MainThread:3777087 [wandb_init.py:init():749] communicating run to backend with 90.0 second timeout
2025-08-23 20:05:52,673 INFO    MainThread:3777087 [wandb_run.py:_on_init():2254] communicating current version
2025-08-23 20:05:53,246 INFO    MainThread:3777087 [wandb_run.py:_on_init():2263] got version response upgrade_message: "wandb version 0.21.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"

2025-08-23 20:05:53,246 INFO    MainThread:3777087 [wandb_init.py:init():800] starting run threads in backend
2025-08-23 20:05:56,917 INFO    MainThread:3777087 [wandb_run.py:_console_start():2233] atexit reg
2025-08-23 20:05:56,918 INFO    MainThread:3777087 [wandb_run.py:_redirect():2088] redirect: wrap_raw
2025-08-23 20:05:56,918 INFO    MainThread:3777087 [wandb_run.py:_redirect():2153] Wrapping output streams.
2025-08-23 20:05:56,918 INFO    MainThread:3777087 [wandb_run.py:_redirect():2178] Redirects installed.
2025-08-23 20:05:56,919 INFO    MainThread:3777087 [wandb_init.py:init():841] run started, returning control to user process
2025-08-23 20:20:09,741 INFO    MainThread:3777087 [wandb_run.py:_finish():1962] finishing run wanweijie2005-/hpt-transfer/inzifh3c
2025-08-23 20:20:09,741 INFO    MainThread:3777087 [wandb_run.py:_atexit_cleanup():2202] got exitcode: 0
2025-08-23 20:20:09,741 INFO    MainThread:3777087 [wandb_run.py:_restore():2185] restore
2025-08-23 20:20:09,741 INFO    MainThread:3777087 [wandb_run.py:_restore():2191] restore done
2025-08-23 20:20:19,215 INFO    MainThread:3777087 [wandb_run.py:_footer_history_summary_info():3837] rendering history
2025-08-23 20:20:19,216 INFO    MainThread:3777087 [wandb_run.py:_footer_history_summary_info():3869] rendering summary
2025-08-23 20:20:19,221 INFO    MainThread:3777087 [wandb_run.py:_footer_sync_info():3796] logging synced files
